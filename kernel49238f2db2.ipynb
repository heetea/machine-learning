{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport lightgbm as lgb\nimport scipy as sp\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.ensemble import RandomForestClassifier\nfrom matplotlib import pyplot\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Only load those columns in order to save space\nkeep_cols = ['event_id', 'game_session', 'installation_id', 'event_count', 'event_code', 'title', 'game_time', 'type', 'world']\ntrain = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv', usecols=keep_cols)\ntest = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv', usecols=keep_cols)\ntrain_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\nsubmission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_assess = test[test.type == 'Assessment'].copy()\ntest_labels = submission.copy()\ntest_labels['title'] = test_labels.installation_id.progress_apply(\n    lambda install_id: test_assess[test_assess.installation_id == install_id].iloc[-1].title\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_game_time_stats(group, col):\n    return group[\n        ['installation_id', col, 'event_count', 'game_time']\n    ].groupby(['installation_id', col]).agg(\n        [np.mean, np.sum, np.std]\n    ).reset_index().pivot(\n        columns=col,\n        index='installation_id'\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_and_reduce(df, df_labels):\n    \"\"\"\n    Author: https://www.kaggle.com/xhlulu/\n    Source: https://www.kaggle.com/xhlulu/ds-bowl-2019-simple-lgbm-using-aggregated-data\n    \"\"\"\n    \n    # First only filter the useful part of the df\n    df = df[df.installation_id.isin(df_labels.installation_id.unique())]\n    \n    # group1 is am intermediary \"game session\" group,\n    # which are reduced to one record by game session. group_game_time takes\n    # the max value of game_time (final game time in a session) and \n    # of event_count (total number of events happened in the session).\n    group_game_time = df.drop(columns=['event_id', 'event_code']).groupby(\n        ['game_session', 'installation_id', 'title', 'type', 'world']\n    ).max().reset_index()\n\n    # group3, group4 are grouped by installation_id \n    # and reduced using summation and other summary stats\n    title_group = (\n        pd.get_dummies(\n            group_game_time.drop(columns=['game_session', 'event_count', 'game_time']),\n            columns=['title', 'type', 'world'])\n        .groupby(['installation_id'])\n        .sum()\n    )\n\n    event_game_time_group = (\n        group_game_time[['installation_id', 'event_count', 'game_time']]\n        .groupby(['installation_id'])\n        .agg([np.sum, np.mean, np.std, np.min, np.max])\n    )\n    \n    # Additional stats on group1\n    world_time_stats = compute_game_time_stats(group_game_time, 'world')\n    type_time_stats = compute_game_time_stats(group_game_time, 'type')\n    \n    return (\n        title_group.join(event_game_time_group)\n        .join(world_time_stats)\n        .join(type_time_stats)\n        .fillna(0)\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_small = group_and_reduce(train, train_labels)\ntest_small = group_and_reduce(test, test_labels)\n\nprint(train_small.shape)\ntrain_small.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = train_labels.title.unique()\ntitle2mode = {}\n\nfor title in titles:\n    mode = train_labels[train_labels.title == title].accuracy_group.value_counts().index[0]\n    title2mode[title] = mode\n\ntrain_labels['title_mode'] = train_labels.title.apply(lambda title: title2mode[title])\ntest_labels['title_mode'] = test_labels.title.apply(lambda title: title2mode[title])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train = pd.get_dummies(\n    (\n        train_labels.set_index('installation_id')\n        .drop(columns=['num_correct', 'num_incorrect', 'accuracy', 'game_session'])\n        .join(train_small)\n    ), \n    columns=['title']\n)\n\n# Experimental: only take the last record of each installation\nfinal_train = final_train.reset_index().groupby('installation_id').apply(lambda x: x.iloc[-1])\nfinal_train = final_train.drop(columns='installation_id')\n\nprint(final_train.shape)\nfinal_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test = pd.get_dummies(test_labels.set_index('installation_id').join(test_small), columns=['title'])\n\nprint(final_test.shape)\nfinal_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lgb_train(X, y, cv, **kwargs):\n    \"\"\"\n    Author: https://www.kaggle.com/xhlulu/\n    Source: https://www.kaggle.com/xhlulu/ds-bowl-2019-simple-lgbm-using-aggregated-data\n    \"\"\"\n    lgb_models = []\n    \n    kf = KFold(n_splits=cv, random_state=2019)\n    \n    for train, test in kf.split(X):\n        x_train, x_val, y_train, y_val = X[train], X[test], y[train], y[test]\n        \n        train_set = lgb.Dataset(x_train, y_train)\n        val_set = lgb.Dataset(x_val, y_val)\n        \n        model = lgb.train(train_set=train_set, valid_sets=[train_set, val_set], **kwargs)\n        lgb_models.append(model)\n        \n        if kwargs.get(\"verbose_eval\"):\n            print(\"\\n\" + \"=\"*50 + \"\\n\")\n    \n    return lgb_models\n\ndef lgb_predict(models, X):\n    \"\"\"\n    Author: https://www.kaggle.com/xhlulu/\n    Source: https://www.kaggle.com/xhlulu/ds-bowl-2019-simple-lgbm-using-aggregated-data\n    \"\"\"\n    return np.mean([model.predict(X) for model in models], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb_train(X, y, cv, **kwargs):\n    \"\"\"\n    Author: https://www.kaggle.com/xhlulu/\n    Source: https://www.kaggle.com/xhlulu/ds-bowl-2019-simple-lgbm-using-aggregated-data\n    \"\"\"\n    xgb_models = []\n    \n    kf = KFold(n_splits=cv, random_state=2019)\n    \n    for train, test in kf.split(X):\n        x_train, x_val, y_train, y_val = X[train], X[test], y[train], y[test]\n        \n        train_set = xgb.DMatrix(x_train, y_train)\n        val_set = xgb.DMatrix(x_val, y_val)\n                \n        model = xgb.train(pars,\n                          train_set,\n                          num_boost_round=1000,\n                          evals=[(train_set, 'train'), (val_set, 'val')],\n                          verbose_eval=500,\n                          early_stopping_rounds=100,\n                         )\n        xgb_models.append(model)\n        \n        if kwargs.get(\"verbose_eval\"):\n            print(\"\\n\" + \"=\"*50 + \"\\n\")\n    \n    return xgb_models\n\ndef xgb_predict(models, X):\n    \"\"\"\n    Author: https://www.kaggle.com/xhlulu/\n    Source: https://www.kaggle.com/xhlulu/ds-bowl-2019-simple-lgbm-using-aggregated-data\n    \"\"\"\n    return np.mean([model.predict(X) for model in models], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = final_train.drop(columns='accuracy_group').values\ny = final_train['accuracy_group'].values\n\nparams = {\n    'learning_rate': 0.01,\n    'bagging_fraction': 0.95,\n    'feature_fraction': 0.2,\n    'max_height': 3,\n    'lambda_l1': 10,\n    'lambda_l2': 10,\n    'metric': 'multiclass',\n    'objective': 'multiclass',\n    'num_classes': 4,\n    'random_state': 2019\n}\n\nlgb_models = lgb_train(X, y, cv=10, params=params, num_boost_round=2000,\n                  early_stopping_rounds=100, verbose_eval=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = final_test.drop(columns=['accuracy_group'])\nlgb_pred = lgb_predict(models=lgb_models, X=X_test).argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pars = {\n    'colsample_bytree': 0.8,                 \n    'learning_rate': 0.01,\n    'max_depth': 10,\n    'subsample': 1,\n    'objective':'multi:softprob',\n    'num_class':4,\n    'eval_metric':'mlogloss',\n    'min_child_weight':3,\n    'gamma':0.25,\n    'n_estimators':500\n}\n\nmodels = xgb_train(X, y, cv=10, num_boost_round=1000, early_stopping_rounds=100, verbose_eval=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_test = xgb.DMatrix(final_test.drop(columns=['accuracy_group']).values)\nxgb_pred = xgb_predict(models=models, X=xgb_test).argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = final_train.drop(columns='accuracy_group')\ny = final_train['accuracy_group'].values\ndf_train_x,df_test_x,df_train_y,df_test_y = train_test_split(X,y, test_size=0.3,random_state=1234)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_score=[]\ntest_score=[]\n\ngf_final =RandomForestClassifier(random_state=2019,n_estimators=170, min_samples_leaf=10, max_depth=34)\ngf_final.fit(df_train_x,df_train_y)\ntrain_score.append(gf_final.score(df_train_x,df_train_y))\n\ntest_score.append(gf_final.score(df_test_x,df_test_y))\nprint(\"Score on training set :\" ,(gf_final.score(df_train_x,df_train_y)).round(3))\n\nprint(\"Score on test set :\" ,(gf_final.score(df_test_x,df_test_y)).round(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred = gf_final.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ens = pd.DataFrame()\nens[\"xgb\"] = xgb_pred\nens[\"lgbm\"] = lgb_pred\nens[\"rf\"] = rf_pred\nens.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = []\nfor i,j,k in  ens.values:\n    if j == k:\n        result.append(j)\n    else:\n        result.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test['accuracy_group'] = result\nfinal_test[['accuracy_group']].to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}